{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'batch_size': 16,\n",
    "    'embedding_dim': 256,\n",
    "    'lstm_out_dim': 512,\n",
    "    'hidden_size': 100,\n",
    "    'epochs': 150,\n",
    "    'learning_rate': 0.001\n",
    "}\n",
    "\n",
    "PADDING_TOKEN = '<PAD>'\n",
    "UNKNOWN_TOKEN = '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '/scratch/lt2316-h18-resources/coco/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.19s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=21.80s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco_captions = COCO(BASE_PATH + 'annotations/captions_train2017.json')\n",
    "coco_instances = COCO(BASE_PATH + 'annotations/instances_train2017.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler():\n",
    "    def __init__(self, coco_captions, number_of_samples=100, train_split=0.8, val_split=0.05, test_split=0.15) -> None:\n",
    "        samples = []\n",
    "        transform = transforms.ToTensor()\n",
    "        \n",
    "        random_images = random.sample(list(coco_captions.imgs.values()), number_of_samples)\n",
    "        for image_info in random_images:\n",
    "            image = Image.open(BASE_PATH + 'train2017/' + image_info['file_name']).resize((100,100)).convert('RGB')\n",
    "            samples.extend([{\n",
    "                'image': transform(image),\n",
    "                'caption': annotation['caption']          \n",
    "            } for annotation in coco_captions.imgToAnns[image_info['id']]])\n",
    "\n",
    "        train_border = int(train_split * number_of_samples)\n",
    "        val_border = int((train_split + val_split) * number_of_samples)\n",
    "\n",
    "        self.train_samples = samples[:train_border]\n",
    "        self.val_samples = samples[train_border:val_border]\n",
    "        self.test_samples = samples[val_border:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCO_Dataset(Dataset):\n",
    "    def __init__(self, samples) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.max_length_context = -1\n",
    "        vocab = {PADDING_TOKEN, UNKNOWN_TOKEN}\n",
    "        for sample in samples:\n",
    "            split_caption = word_tokenize(sample['caption'])\n",
    "            vocab.update(split_caption)\n",
    "            self.max_length_context = max(self.max_length_context, len(split_caption))\n",
    "\n",
    "        self.vocab = {word: index for index, word in enumerate(list(vocab))}\n",
    "        self.samples = []\n",
    "        for sample in samples:\n",
    "            split_caption = word_tokenize(sample['caption'])\n",
    "            padded_context = [self.get_encoded_word(word) for word in split_caption]\n",
    "            padded_context.extend([self.get_encoded_word(PADDING_TOKEN)] * (self.max_length_context - len(split_caption)))\n",
    "            self.samples.append({\n",
    "                'image': sample['image'],\n",
    "                'caption': sample['caption'],\n",
    "                'encoded_caption': torch.tensor(padded_context)\n",
    "            })\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.samples[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def get_encoded_word(self, word):\n",
    "        if word in self.vocab:\n",
    "            return self.vocab[word]\n",
    "        else:\n",
    "            return self.vocab[UNKNOWN_TOKEN]\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = Sampler(coco_captions)\n",
    "\n",
    "train_dataloader = DataLoader(COCO_Dataset(sampler.train_samples),\n",
    "                              batch_size=hyperparameters['batch_size'],\n",
    "                              shuffle=True)\n",
    "val_dataloader = DataLoader(COCO_Dataset(sampler.val_samples),\n",
    "                            batch_size=hyperparameters['batch_size'],\n",
    "                            shuffle=True)\n",
    "test_dataloader = DataLoader(COCO_Dataset(sampler.test_samples),\n",
    "                             batch_size=hyperparameters['batch_size'],\n",
    "                             shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': tensor([[[0.4745, 0.4706, 0.4078,  ..., 0.4471, 0.4353, 0.4588],\n",
       "          [0.4588, 0.4667, 0.4275,  ..., 0.4471, 0.4392, 0.4588],\n",
       "          [0.4667, 0.4706, 0.4118,  ..., 0.4588, 0.4431, 0.4588],\n",
       "          ...,\n",
       "          [0.5176, 0.6392, 0.7255,  ..., 0.7529, 0.7529, 0.7373],\n",
       "          [0.5137, 0.6157, 0.7255,  ..., 0.7529, 0.7529, 0.7490],\n",
       "          [0.5255, 0.6314, 0.7255,  ..., 0.7412, 0.7373, 0.7412]],\n",
       " \n",
       "         [[0.4549, 0.4549, 0.4039,  ..., 0.4471, 0.4392, 0.4549],\n",
       "          [0.4471, 0.4549, 0.4157,  ..., 0.4431, 0.4392, 0.4588],\n",
       "          [0.4549, 0.4588, 0.3961,  ..., 0.4588, 0.4471, 0.4588],\n",
       "          ...,\n",
       "          [0.5490, 0.5765, 0.6039,  ..., 0.5490, 0.5412, 0.5255],\n",
       "          [0.5490, 0.5647, 0.6118,  ..., 0.5451, 0.5373, 0.5333],\n",
       "          [0.5608, 0.5725, 0.5961,  ..., 0.5373, 0.5333, 0.5294]],\n",
       " \n",
       "         [[0.4706, 0.4627, 0.4078,  ..., 0.4471, 0.4471, 0.4627],\n",
       "          [0.4471, 0.4549, 0.4275,  ..., 0.4510, 0.4471, 0.4667],\n",
       "          [0.4588, 0.4627, 0.4039,  ..., 0.4588, 0.4510, 0.4706],\n",
       "          ...,\n",
       "          [0.6000, 0.5765, 0.6118,  ..., 0.5529, 0.5451, 0.5333],\n",
       "          [0.6118, 0.5647, 0.6039,  ..., 0.5451, 0.5451, 0.5451],\n",
       "          [0.6275, 0.5804, 0.5922,  ..., 0.5373, 0.5373, 0.5333]]]),\n",
       " 'caption': 'A man holding a tennis racquet on top of a tennis court.',\n",
       " 'encoded_caption': tensor([ 22, 186,  35, 117,  97,  54, 169,   9, 177, 117,  97,  83, 130,  47,\n",
       "          47,  47,  47,  47,  47,  47,  47,  47,  47,  47,  47,  47,  47])}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, out_dim, padding_idx) -> None:\n",
    "        super(CaptionEncoder, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.rnn = nn.LSTM(embedding_dim, out_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "\n",
    "    def forward(self, caption_batch):\n",
    "        embeddings = self.embeddings(caption_batch)\n",
    "        _, (h_n, _) = self.rnn(embeddings)\n",
    "\n",
    "        return h_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        \n",
    "        self.image_encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 3, 3),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, image_batch):\n",
    "        return self.image_encoder(image_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionEvaluator(nn.Module):\n",
    "    def __init__(self, caption_encoder, image_encoder, hidden_size) -> None:\n",
    "        super(CaptionEvaluator, self).__init__()\n",
    "\n",
    "        self.caption_encoder = caption_encoder\n",
    "        self.image_encoder = image_encoder\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(100 * 100 * 3, hidden_size),\n",
    "            nn.Dropout(0.05),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, int(hidden_size/2)),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(int(hidden_size/2), 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, image_batch, caption_batch):\n",
    "        encoded_caption = self.caption_encoder(caption_batch)\n",
    "        flattened_caption = torch.flatten(encoded_caption.permute(1,0,2), 1)\n",
    "\n",
    "        encoded_image = self.image_encoder(image_batch)\n",
    "        flattened_image = torch.flatten(encoded_image, 1)\n",
    "\n",
    "        concatenated_encoding = torch.cat((flattened_caption, flattened_image), 1)\n",
    "        print(concatenated_encoding.shape)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_encoder = CaptionEncoder(train_dataloader.dataset.get_vocab_size(),\n",
    "                                 hyperparameters['embedding_dim'],\n",
    "                                 hyperparameters['lstm_out_dim'],\n",
    "                                 train_dataloader.dataset.get_encoded_word(PADDING_TOKEN))\n",
    "\n",
    "image_encoder = ImageEncoder()\n",
    "caption_evaluator = CaptionEvaluator(caption_encoder,\n",
    "                                     image_encoder, \n",
    "                                     hyperparameters['hidden_size'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 7203])\n",
      "torch.Size([16, 1024])\n",
      "torch.Size([16, 8227])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_dataloader):\n",
    "#    image = image_encoder(batch['image'])\n",
    "    output = caption_evaluator(batch['image'], batch['encoded_caption'])\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('lt2326_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd2feef55d4ee460479306887fa1cc2179f52f92e40ccb13d5bbd3870fa5c6f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
